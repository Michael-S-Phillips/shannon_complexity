{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process large images via tiling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dimensions: 62374x47889\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rasterio.windows import Window\n",
    "import rasterio\n",
    "\n",
    "# base_img_path = '/mnt/nili_e/Software/fractal_complexity/data/Jezero_HiRISE_clipped.tif'\n",
    "base_img_path = '/mnt/holuhraun/spiny_pond.tif'\n",
    "\n",
    "# Define tile size and overlap\n",
    "tile_size = 500\n",
    "overlap = int(tile_size * 0.1)  # 10% overlap\n",
    "\n",
    "# Open the image to get its dimensions\n",
    "with rasterio.open(base_img_path) as src:\n",
    "    img_width = src.width\n",
    "    img_height = src.height\n",
    "    print(f\"Image dimensions: {img_width}x{img_height}\")\n",
    "    \n",
    "    # Create output directory for tiles\n",
    "    # output_dir = \"/mnt/nili_e/Software/fractal_complexity/data/Jezero_HiRISE_clipped_tiles/\"\n",
    "    output_dir = \"/mnt/holuhraun/spiny_pond_tiles/\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Loop through the image and save tiles\n",
    "    for i in range(0, img_height, tile_size - overlap):\n",
    "        for j in range(0, img_width, tile_size - overlap):\n",
    "            if i > 2:\n",
    "                break\n",
    "            # Define the window for the current tile\n",
    "            window = Window(j, i, tile_size, tile_size)\n",
    "            \n",
    "            # Adjust the window size to avoid going out of bounds\n",
    "            window = window.intersection(Window(0, 0, img_width, img_height))\n",
    "            \n",
    "            # Read the tile\n",
    "            tile = src.read(1, window=window)\n",
    "            \n",
    "            # Define the output path\n",
    "            tile_filename = f\"Jezero_HiRISE_clipped_tile_{i}_{j}.tif\"\n",
    "            tile_path = os.path.join(output_dir, tile_filename)\n",
    "            \n",
    "            # # Save the tile\n",
    "            # profile = src.profile\n",
    "            # profile.update({\n",
    "            #     \"height\": window.height,\n",
    "            #     \"width\": window.width,\n",
    "            #     \"transform\": rasterio.windows.transform(window, src.transform)\n",
    "            # }) \n",
    "            \n",
    "            # with rasterio.open(tile_path, \"w\", **profile) as dst:\n",
    "            #     dst.write(tile, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code outline\n",
    "\n",
    "### inputs\n",
    "- path to large image\n",
    "- path to shape file or gdb (with layer name)\n",
    "- shape file unit name\n",
    "\n",
    "### process\n",
    "- create a tile scheme with overlap\n",
    "- fetch the window from the raster\n",
    "- fetch the corresponding window from the shape file\n",
    "- process complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import numpy as np\n",
    "from rasterio.features import rasterize\n",
    "import fiona\n",
    "from shapely.geometry import mapping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_windows(img_path, tile_size, overlap):\n",
    "    # Open the image to get its dimensions\n",
    "    window_list = []\n",
    "    with rasterio.open(img_path) as src:\n",
    "        img_width = src.width\n",
    "        img_height = src.height\n",
    "        print(f\"Image dimensions: {img_width}x{img_height}\")\n",
    "        \n",
    "        # Loop through the image and save tiles\n",
    "        for i in range(0, img_height, tile_size - overlap):\n",
    "            for j in range(0, img_width, tile_size - overlap):\n",
    "                # Define the window for the current tile\n",
    "                window = Window(j, i, tile_size, tile_size)\n",
    "                \n",
    "                # Adjust the window size to avoid going out of bounds\n",
    "                window = window.intersection(Window(0, 0, img_width, img_height))\n",
    "                window_list.append(window)\n",
    "    \n",
    "    return window_list\n",
    "\n",
    "def get_tile_window(image_path, window):\n",
    "    \"\"\"\n",
    "    Get a tile from the image using a specified window.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the image file\n",
    "    window : rasterio.windows.Window\n",
    "        Window object specifying the tile location and size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tile : numpy.ndarray\n",
    "        2D array containing the tile data\n",
    "    transform : affine.Affine\n",
    "        Affine transformation for the raster\n",
    "    \"\"\"\n",
    "    with rasterio.open(image_path) as src:\n",
    "        # Adjust the window size to avoid going out of bounds\n",
    "        window = window.intersection(Window(0, 0, img_width, img_height))\n",
    "        # Read the tile\n",
    "        tile = src.read(1, window=window)\n",
    "        src_transform = src.transform\n",
    "        window_transform = rasterio.windows.transform(window, src.transform)\n",
    "        tile_extent = src.window_bounds(window)\n",
    "        tile_crs = src.crs\n",
    "    \n",
    "    return tile, src_transform, window_transform, tile_extent, tile_crs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from skimage.util import view_as_windows\n",
    "from skimage.measure import shannon_entropy\n",
    "\n",
    "def kernel_entropty(kernel_sizes, image_path, window, out_dir = None, crop = False):\n",
    "    base_img, src_transform, window_transform, tile_extent, tile_crs = get_tile_window(image_path, window)\n",
    "    complexity_map_cube = np.zeros((base_img.shape[0], base_img.shape[1], len(kernel_sizes)), dtype=np.float32)\n",
    "\n",
    "    for b, kernel_size in tqdm(enumerate(kernel_sizes)):\n",
    "        # Initialize an array to store the complexity values\n",
    "        kernel_complexity_map = np.zeros_like(base_img)\n",
    "        # Create a sliding window view of the padded map\n",
    "        kernels = view_as_windows(base_img, (kernel_size, kernel_size))\n",
    "\n",
    "        # Iterate over each window and calculate Shannon entropy\n",
    "        for i in range(kernel_complexity_map.shape[0]):\n",
    "            for j in range(kernel_complexity_map.shape[1]):\n",
    "                kernel = kernels[i, j]\n",
    "                kernel_complexity_map[i, j] = shannon_entropy(kernel)\n",
    "\n",
    "        # mask zeros\n",
    "        # kernel_complexity_map = np.ma.masked_where(kernel_complexity_map == 0, kernel_complexity_map)\n",
    "\n",
    "        complexity_map_cube[:, :, b] = kernel_complexity_map\n",
    "\n",
    "    if out_dir is not None:\n",
    "        # Save the complexity map as a GeoTIFF\n",
    "        output_path = os.path.join(out_dir, f\"complexity_map_{window.col_off}_{window.row_off}.tif\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        if crop:\n",
    "            # Crop the output to the original tile size\n",
    "            complexity_map_cube = complexity_map_cube[window.row_off:window.row_off + window.height,\n",
    "                                                      window.col_off:window.col_off + window.width]\n",
    "            window_transform = rasterio.windows.transform(window, src_transform)\n",
    "            \n",
    "        with rasterio.open(output_path, 'w', driver='GTiff', height=complexity_map_cube.shape[0],\n",
    "                           width=complexity_map_cube.shape[1], count=len(kernel_sizes),\n",
    "                           dtype=complexity_map_cube.dtype, crs=tile_crs,\n",
    "                           transform=window_transform) as dst:\n",
    "            for b in range(len(kernel_sizes)):\n",
    "                dst.write(complexity_map_cube[:, :, b], b + 1)\n",
    "    else:\n",
    "        return complexity_map_cube\n",
    "\n",
    "\n",
    "base_img_path = '/mnt/nili_e/Software/fractal_complexity/data/Jezero_HiRISE_clipped.tif'\n",
    "kernel_sizes = [2, 3, 5, 7, 9, 11, 15, 19, 25, 31, 43, 55, 75]  \n",
    "\n",
    "# Define tile size and overlap\n",
    "tile_size = 1024\n",
    "overlap = max(int(tile_size * 0.1), max(kernel_sizes))  # 10% overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dimensions: 1256x980\n",
      "Processing 1 tiles in batches of 3\n",
      "Using 24 CPU workers for processing.\n",
      "Processing batch 1/1 (1 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles: 100%|██████████| 1/1 [27:09<00:00, 1629.75s/it]\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import tiled_complexity\n",
    "reload(tiled_complexity)\n",
    "from tiled_complexity import tiledComplexity\n",
    "\n",
    "# base_img_path = '/mnt/nili_e/Software/fractal_complexity/data/Jezero_HiRISE_clipped.tif'\n",
    "# out_dir = '/mnt/nili_e/Software/fractal_complexity/data/Jezero_HiRISE_complexity_tiles/'\n",
    "\n",
    "# base_img_path = '/mnt/holuhraun/t5_dsm_2015_50cm_clip.tif'\n",
    "# out_dir = '/mnt/holuhraun/t5_dsm_50cm_complexity_tiles/'\n",
    "\n",
    "base_img_path = '/mnt/fagradalsfjall/sites_abc_clip.tif'\n",
    "out_dir = '/mnt/fagradalsfjall/sites_abc_clip_complexity/'\n",
    "\n",
    "kernel_sizes = [3, 4, 5, 7, 9, 11, 15, 19, 25, 31, 43, 55, 75]  \n",
    "\n",
    "# Define tile size and overlap\n",
    "tile_size = 2048\n",
    "overlap = max(int(tile_size * 0.1), max(kernel_sizes))  # 10% overlap\n",
    "\n",
    "tc_processor = tiledComplexity(base_img_path, tile_size, overlap, kernel_sizes, out_dir=out_dir, crop=True)\n",
    "# tc_processor.visualize_tiles() \n",
    "tc_processor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 240 tile files to merge\n",
      "Processing tiles in batches of 198 to fit within 4GB memory limit\n",
      "Calculating final mosaic bounds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling bounds:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling bounds: 100%|██████████| 100/100 [00:01<00:00, 90.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 1 of 13 (kernel size: 3)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:58<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:13<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 2 of 13 (kernel size: 4)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:35<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:14<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 3 of 13 (kernel size: 5)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:25<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:12<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 4 of 13 (kernel size: 7)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:26<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:11<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 5 of 13 (kernel size: 9)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:23<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:09<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 6 of 13 (kernel size: 11)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:22<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:11<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 7 of 13 (kernel size: 15)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:23<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:12<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 8 of 13 (kernel size: 19)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:26<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:10<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 9 of 13 (kernel size: 25)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:27<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:13<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 10 of 13 (kernel size: 31)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:32<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:12<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 11 of 13 (kernel size: 43)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:33<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:11<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 12 of 13 (kernel size: 55)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:31<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:11<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing band 13 of 13 (kernel size: 75)\n",
      "  Processing batch 1/2 (198 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 198/198 [05:33<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/2 (42 tiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening tiles: 100%|██████████| 42/42 [01:12<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosaic successfully created: /mnt/holuhraun/t5_dsm_50cm_complexity_tiles/t5_dsm_50cm_complexity_mosaic.tif\n",
      "Dimensions: 22123x24074, 13 bands\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/holuhraun/t5_dsm_50cm_complexity_tiles/t5_dsm_50cm_complexity_mosaic.tif'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_path='/mnt/nili_e/Software/fractal_complexity/data/Jezero_HiRISE_complexity_mosaic/complexity_mosaic.tif'\n",
    "tc_processor.create_mosaic(output_path='/mnt/holuhraun/t5_dsm_50cm_complexity_tiles/t5_dsm_50cm_complexity_mosaic.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_shp_window(shp_path, unit_name, window, window_transform, layer_name = None):\n",
    "    \"\"\"\n",
    "    Get a tile from the shapefile using a specified window.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shp_path : str\n",
    "        Path to the shapefile\n",
    "    unit_name : str\n",
    "        Name of the column containing facies information\n",
    "    window : rasterio.windows.Window\n",
    "        Window object specifying the tile location and size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tile : numpy.ndarray\n",
    "        2D array containing the tile data\n",
    "    transform : affine.Affine\n",
    "        Affine transformation for the raster\n",
    "    \"\"\"\n",
    "    # Load the shapefile\n",
    "    if shp_path.endswith('.gdb'):\n",
    "        with fiona.open(shp_path, layer_name) as src:\n",
    "            shapes = [(feature[\"geometry\"], feature['properties'][unit_name]) for feature in src]\n",
    "    else:\n",
    "        with fiona.open(shp_path) as src:\n",
    "            shapes = [(feature[\"geometry\"], feature['properties'][unit_name]) for feature in src]\n",
    "    \n",
    "    # Rasterize the shapefile for the specified window\n",
    "    facies_map = rasterize(\n",
    "        shapes,\n",
    "        out_shape=(window.height, window.width),\n",
    "        transform=window_transform,\n",
    "        all_touched=True,\n",
    "        fill=0,  # Background value\n",
    "        dtype=np.int32  # Use np.int32 to handle larger range of values\n",
    "    )\n",
    "    \n",
    "    return facies_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import math\n",
    "import tqdm\n",
    "from skimage.measure import shannon_entropy\n",
    "from skimage import exposure\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "def load_base_image(image_path, band=1, img_nan_value = None):\n",
    "    \"\"\"\n",
    "    Load a base image (elevation or grayscale data)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the image file\n",
    "    band : int\n",
    "        Band number to read (default is 1 for single band images)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    image : numpy.ndarray\n",
    "        2D array containing the image data\n",
    "    transform : affine.Affine\n",
    "        Affine transformation for the raster\n",
    "    \"\"\"\n",
    "    with rasterio.open(image_path) as src:\n",
    "        image = src.read(band)\n",
    "        if img_nan_value is not None:\n",
    "            image = np.where(image == img_nan_value, np.nan, image)  # Replace specified value with NaN\n",
    "        transform = src.transform\n",
    "    \n",
    "    return image, transform\n",
    "\n",
    "def calculate_facies_entropy(base_image, facies_map, norm = False):\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy for each facies in the facies map\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_image : numpy.ndarray\n",
    "        2D array of the base image (elevation or grayscale data)\n",
    "    facies_map : numpy.ndarray\n",
    "        2D array where each unique integer represents a different facies\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    facies_entropy : dict\n",
    "        Dictionary mapping facies IDs to their Shannon entropy values\n",
    "    entropy_map : numpy.ndarray\n",
    "        2D array containing the entropy value for each pixel based on its facies\n",
    "    \"\"\"\n",
    "    # Get unique facies values (excluding 0 if it's a no-data value)\n",
    "    unique_facies = np.unique(facies_map)\n",
    "    if 0 in unique_facies and len(unique_facies) > 1:\n",
    "        unique_facies = unique_facies[unique_facies != 0]\n",
    "    \n",
    "    facies_entropy = {}\n",
    "    entropy_map = np.zeros_like(facies_map, dtype=float)\n",
    "    facies_entropy_maps = {}\n",
    "    \n",
    "    # Calculate Shannon entropy for each facies\n",
    "    for facies_id in unique_facies:\n",
    "        # Create mask for this facies\n",
    "        mask = facies_map == facies_id\n",
    "        \n",
    "        if np.sum(mask) > 0:  # Only calculate if facies exists\n",
    "            # Extract the base image values for this facies\n",
    "            facies_values = base_image[mask]\n",
    "            # drop NaN values if any\n",
    "            facies_values = facies_values[~np.isnan(facies_values)]\n",
    "            if facies_values.size == 0:\n",
    "                continue\n",
    "            \n",
    "            # Normalize values to 0-255 for entropy calculation\n",
    "            if np.nanmin(facies_values) != np.nanmax(facies_values):\n",
    "                if norm:\n",
    "                    facies_values_norm = exposure.rescale_intensity(facies_values, out_range=(0, 255)).astype(np.uint8)\n",
    "                    \n",
    "                    # Calculate Shannon entropy\n",
    "                    facies_entropy[facies_id] = shannon_entropy(facies_values_norm)\n",
    "                else:                    \n",
    "                    # Calculate Shannon entropy without normalization\n",
    "                    facies_entropy[facies_id] = shannon_entropy(facies_values)\n",
    "            else:\n",
    "                # If all values are the same, entropy is 0\n",
    "                facies_entropy[facies_id] = 0.0\n",
    "            \n",
    "            # Create a local entropy map for this facies\n",
    "            local_entropy_map = np.zeros_like(base_image, dtype=float)\n",
    "            \n",
    "            # For each pixel in this facies, calculate local entropy in a window\n",
    "            window_size = 5  # Use a 5x5 window for local entropy\n",
    "            half_window = window_size // 2\n",
    "            \n",
    "            rows, cols = np.where(mask)\n",
    "            for i, j in zip(rows, cols):\n",
    "                # Define window boundaries with border handling\n",
    "                row_min = max(0, i - half_window)\n",
    "                row_max = min(base_image.shape[0], i + half_window + 1)\n",
    "                col_min = max(0, j - half_window)\n",
    "                col_max = min(base_image.shape[1], j + half_window + 1)\n",
    "                \n",
    "                # Extract window values\n",
    "                window = base_image[row_min:row_max, col_min:col_max]\n",
    "                # drop NaN values if any\n",
    "                window = window[~np.isnan(window)]\n",
    "                if window.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Normalize window values\n",
    "                if np.nanmin(window) != np.nanmax(window):\n",
    "                    if norm:\n",
    "                        window_norm = exposure.rescale_intensity(window, out_range=(0, 255)).astype(np.uint8)\n",
    "                        # Calculate Shannon entropy for the window\n",
    "                        local_entropy_map[i, j] = shannon_entropy(window_norm)\n",
    "                    else:\n",
    "                        # Calculate Shannon entropy without normalization\n",
    "                        local_entropy_map[i, j] = shannon_entropy(window)\n",
    "                else:\n",
    "                    local_entropy_map[i, j] = 0.0\n",
    "            \n",
    "            # Store the local entropy map for this facies\n",
    "            facies_entropy_maps[facies_id] = local_entropy_map\n",
    "            \n",
    "            # Assign entropy values to the entropy map\n",
    "            entropy_map[mask] = local_entropy_map[mask]\n",
    "    \n",
    "    return facies_entropy, entropy_map, facies_entropy_maps\n",
    "\n",
    "def calculate_complexity_map(facies_map, facies_entropy, pixel_size=1.0, kernel_radius=10):\n",
    "    \"\"\"\n",
    "    Calculate the complexity (information content) map from a facies map,\n",
    "    incorporating the entropy within each facies.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    facies_map : numpy.ndarray\n",
    "        2D array where each unique integer represents a different facies\n",
    "    facies_entropy : dict\n",
    "        Dictionary mapping facies IDs to their Shannon entropy values\n",
    "    pixel_size : float\n",
    "        Size of each pixel in map units (e.g., meters)\n",
    "    kernel_radius : float\n",
    "        Radius of the kernel for calculating complexity in map units\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    complexity_map : numpy.ndarray\n",
    "        2D array containing the complexity (in bits) at each location\n",
    "    \"\"\"\n",
    "    # Get unique facies values (excluding 0 if it's a no-data value)\n",
    "    unique_facies = np.unique(facies_map)\n",
    "    if 0 in unique_facies and len(unique_facies) > 1:\n",
    "        unique_facies = unique_facies[unique_facies != 0]\n",
    "    \n",
    "    n_facies = len(unique_facies)\n",
    "    rows, cols = facies_map.shape\n",
    "    \n",
    "    # Initialize complexity map\n",
    "    complexity_map = np.zeros((rows, cols), dtype=float)\n",
    "    \n",
    "    # Create distance maps for each facies\n",
    "    distance_maps = {}\n",
    "    for facies_id in unique_facies:\n",
    "        # Create binary map for this facies\n",
    "        binary_map = (facies_map == facies_id).astype(int)\n",
    "        \n",
    "        # Calculate distance to nearest facies boundary\n",
    "        # First get distance from non-facies areas to facies\n",
    "        dist_inside = distance_transform_edt(binary_map) * pixel_size\n",
    "        # Then get distance from facies to non-facies areas\n",
    "        dist_outside = distance_transform_edt(1 - binary_map) * pixel_size\n",
    "        \n",
    "        # Combine: negative inside facies, positive outside\n",
    "        distance_maps[facies_id] = dist_outside - dist_inside\n",
    "    \n",
    "    # Calculate complexity at each point based on kernel\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            # Skip if no data\n",
    "            if facies_map[i, j] == 0 and 0 not in unique_facies:\n",
    "                continue\n",
    "            \n",
    "            # Calculate proximity weights for each facies\n",
    "            weights = np.zeros(n_facies)\n",
    "            entropy_weights = np.zeros(n_facies)\n",
    "            \n",
    "            for k, facies_id in enumerate(unique_facies):\n",
    "                # Get distance to this facies\n",
    "                distance = abs(distance_maps[facies_id][i, j])\n",
    "                \n",
    "                # Apply inverse distance weighting\n",
    "                if distance <= kernel_radius:  # Using kernel_radius directly (in map units)\n",
    "                    # Weight is inversely proportional to distance\n",
    "                    # Add small constant to avoid division by zero\n",
    "                    weights[k] = 1.0 / (distance + 0.1)\n",
    "                    \n",
    "                    # Incorporate facies entropy into the weights\n",
    "                    entropy_factor = facies_entropy.get(facies_id, 0) + 0.1  # Add small constant\n",
    "                    entropy_weights[k] = weights[k] * entropy_factor\n",
    "            \n",
    "            # Normalize entropy-weighted weights to get probability distribution\n",
    "            if np.sum(entropy_weights) > 0:\n",
    "                probabilities = entropy_weights / np.sum(entropy_weights)\n",
    "                \n",
    "                # Calculate Shannon entropy (in bits)\n",
    "                entropy = 0\n",
    "                for p in probabilities:\n",
    "                    if p > 0:\n",
    "                        entropy -= p * math.log2(p)\n",
    "                \n",
    "                complexity_map[i, j] = entropy\n",
    "    \n",
    "    return complexity_map\n",
    "\n",
    "def plot_facies_and_complexity(facies_map, complexity_map, facies_entropy_map=None, base_image=None, \n",
    "                               transform=None, facies_names=None, title=\"Facies Complexity Analysis\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plot the facies map, intra-facies entropy map, and inter-facies complexity map overlaid on a basemap\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    facies_map : numpy.ndarray\n",
    "        2D array where each unique integer represents a different facies\n",
    "    complexity_map : numpy.ndarray\n",
    "        2D array containing the complexity (in bits) at each location\n",
    "    facies_entropy_map : numpy.ndarray, optional\n",
    "        2D array containing the entropy within each facies\n",
    "    base_image : numpy.ndarray, optional\n",
    "        Base image (elevation or grayscale data) to use as the background\n",
    "    transform : affine.Affine, optional\n",
    "        Affine transformation for the raster\n",
    "    facies_names : dict, optional\n",
    "        Dictionary mapping facies IDs to their original names\n",
    "    title : str\n",
    "        Title for the plot\n",
    "    \"\"\"\n",
    "    if facies_entropy_map is None:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 14))\n",
    "    else:\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(28, 12))\n",
    "    \n",
    "    # Prepare basemap (normalize the base image for display)\n",
    "    if base_image is not None:\n",
    "        base_image = np.where(base_image < -5500, np.nan, base_image)  # Replace 0 with NaN for display\n",
    "        # Normalize base_image to 0-1 for display\n",
    "        base_image = exposure.rescale_intensity(base_image, in_range=(np.nanpercentile(base_image, 1), np.nanpercentile(base_image, 99)), out_range=(0, 1))\n",
    "        base_norm = (base_image - np.nanmin(base_image)) / (np.nanmax(base_image) - np.nanmin(base_image))\n",
    "    \n",
    "    # Prepare facies map with custom colormap\n",
    "    unique_facies = np.unique(facies_map)\n",
    "    if 0 in unique_facies and len(unique_facies) > 1:\n",
    "        unique_facies = unique_facies[unique_facies != 0]\n",
    "    \n",
    "    n_facies = len(unique_facies)\n",
    "    colors_list = plt.cm.tab20(np.linspace(0, 1, n_facies))\n",
    "    facies_cmap = ListedColormap(colors_list)\n",
    "    \n",
    "    # Create facies mask for overlay (to show background through transparent areas)\n",
    "    facies_mask = facies_map > 0  # True where facies exist\n",
    "    masked_facies = np.ma.masked_where(~facies_mask, facies_map)\n",
    "    \n",
    "    # Plot 1: Facies Map overlaid on basemap\n",
    "    if base_image is not None:\n",
    "        ax1.imshow(base_norm, cmap='gray')\n",
    "        facies_img = ax1.imshow(masked_facies, cmap=facies_cmap, interpolation='nearest', alpha=0.55)\n",
    "    else:\n",
    "        facies_img = ax1.imshow(facies_map, cmap=facies_cmap, interpolation='nearest')\n",
    "    \n",
    "    ax1.set_title(\"Geological Unit Map\", fontsize=36, pad=20)\n",
    "    ax1.set_xlabel(\"Easting (pixels)\", fontsize=30, labelpad=15)\n",
    "    ax1.set_ylabel(\"Northing (pixels)\", fontsize=30, labelpad=15)\n",
    "    ax1.tick_params(axis='both', labelsize=30)\n",
    "    \n",
    "    bounds = np.arange(0, n_facies + 1) - 0.5\n",
    "    norm = BoundaryNorm(bounds, facies_cmap.N)\n",
    "    cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=facies_cmap), ax=ax1, label=\"Units\", ticks=np.arange(n_facies), fraction=0.046, pad=0.04)\n",
    "    cbar.ax.tick_params(labelsize=20)  # Increase font size for colorbar ticks\n",
    "    cbar.set_label(\"Units\", fontsize=20)  # Increase font size for the label\n",
    "    \n",
    "    if facies_names:\n",
    "        labels = [facies_names.get(int(facies_id), f\"{facies_id}\") for facies_id in unique_facies]\n",
    "        cbar.ax.set_yticklabels(labels, fontsize=20)  # Increase font size for colorbar labels\n",
    "        if any(len(str(label)) > 10 for label in labels):\n",
    "            cbar.ax.tick_params(labelsize=14)\n",
    "    \n",
    "    # Plot 2: Complexity Map overlaid on basemap\n",
    "    if base_image is not None:\n",
    "        ax2.imshow(base_norm, cmap='gray')\n",
    "        masked_complexity = np.ma.masked_where(facies_map == 0, complexity_map)\n",
    "        complexity_img = ax2.imshow(masked_complexity, cmap='plasma', alpha=0.55)\n",
    "    else:\n",
    "        complexity_img = ax2.imshow(complexity_map, cmap='plasma')\n",
    "    \n",
    "    ax2.set_title(\"Inter-Unit Complexity Map\", fontsize=36, pad=20)\n",
    "    ax2.set_xlabel(\"Easting (pixels)\", fontsize=30, labelpad=15)\n",
    "    ax2.tick_params(axis='both', labelsize=30)\n",
    "    cbar2 = plt.colorbar(complexity_img, ax=ax2, label=\"Shannon Entropy (bits)\", fraction=0.046, pad=0.04)\n",
    "    cbar2.ax.tick_params(labelsize=20)  # Increase font size for colorbar ticks\n",
    "    cbar2.set_label(\"Shannon Entropy (bits)\", fontsize=20)  # Increase font size for the label\n",
    "    \n",
    "    # Plot 3: Entropy Map overlaid on basemap (if provided)\n",
    "    if facies_entropy_map is not None:\n",
    "        if base_image is not None:\n",
    "            ax3.imshow(base_norm, cmap='gray')\n",
    "            masked_entropy = np.ma.masked_where(facies_map == 0, facies_entropy_map)\n",
    "            entropy_img = ax3.imshow(masked_entropy, cmap='plasma', alpha=0.8)\n",
    "        else:\n",
    "            entropy_img = ax3.imshow(facies_entropy_map, cmap='plasma')\n",
    "        \n",
    "        ax3.set_title(\"Raster-based Complexity Map\", fontsize=36, pad=20)\n",
    "        ax3.set_xlabel(\"Easting (pixels)\", fontsize=30, labelpad=15)\n",
    "        ax3.tick_params(axis='both', labelsize=30)\n",
    "        cbar3 = plt.colorbar(entropy_img, ax=ax3, label=\"Shannon Entropy (bits)\", fraction=0.046, pad=0.04)\n",
    "        cbar3.ax.tick_params(labelsize=20)  # Increase font size for colorbar ticks\n",
    "        cbar3.set_label(\"Shannon Entropy (bits)\", fontsize=20)  # Increase font size for the label\n",
    "    # plt.suptitle(title, fontsize=38)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format='svg', bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_facies_entropy_summary(facies_map, facies_entropy, facies_names=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot a summary of entropy values for each facies\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    facies_map : numpy.ndarray\n",
    "        2D array where each unique integer represents a different facies\n",
    "    facies_entropy : dict\n",
    "        Dictionary mapping facies IDs to their Shannon entropy values\n",
    "    facies_names : dict, optional\n",
    "        Dictionary mapping facies IDs to their original names\n",
    "    \"\"\"\n",
    "    # Get unique facies values (excluding 0 if it's a no-data value)\n",
    "    unique_facies = np.unique(facies_map)\n",
    "    if 0 in unique_facies and len(unique_facies) > 1:\n",
    "        unique_facies = unique_facies[unique_facies != 0]\n",
    "    \n",
    "    facies_ids = []\n",
    "    entropy_values = []\n",
    "    facies_areas = []\n",
    "    facies_labels = []\n",
    "    \n",
    "    for facies_id in unique_facies:\n",
    "        if facies_id in facies_entropy:\n",
    "            facies_ids.append(facies_id)\n",
    "            entropy_values.append(facies_entropy[facies_id])\n",
    "            facies_areas.append(np.sum(facies_map == facies_id))\n",
    "            \n",
    "            # Use original name if available\n",
    "            if facies_names and facies_id in facies_names:\n",
    "                facies_labels.append(facies_names[facies_id])\n",
    "            else:\n",
    "                facies_labels.append(f\"{facies_id}\")\n",
    "    \n",
    "    # Normalize areas for bubble size\n",
    "    max_area = max(facies_areas)\n",
    "    normalized_areas = [500 * (area / max_area) for area in facies_areas]\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    # Create scatter plot with bubble size proportional to facies area\n",
    "    scatter = plt.scatter(range(len(facies_ids)), entropy_values, s=normalized_areas, \n",
    "                         alpha=0.6, c=facies_ids, cmap='tab20')\n",
    "    \n",
    "    plt.title(\"Shannon Entropy by Unit\")\n",
    "    plt.xlabel(\"Unit\")\n",
    "    plt.ylabel(\"Shannon Entropy (bits)\")\n",
    "    \n",
    "    # Set x-ticks to use facies names instead of IDs\n",
    "    plt.xticks(range(len(facies_ids)), facies_labels, rotation=45, ha='right')\n",
    "    \n",
    "    # Adjust layout to accommodate rotated labels\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    \n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add a size legend\n",
    "    sizes = [0.25, 0.5, 0.75, 1.0]\n",
    "    size_labels = []\n",
    "    for size in sizes:\n",
    "        area = max_area * size\n",
    "        # Format the area value based on its magnitude\n",
    "        if area >= 1e6:\n",
    "            size_labels.append(f\"{area/1e6:.1f}M pixels\")\n",
    "        elif area >= 1e3:\n",
    "            size_labels.append(f\"{area/1e3:.1f}K pixels\")\n",
    "        else:\n",
    "            size_labels.append(f\"{int(area)} pixels\")\n",
    "            \n",
    "    size_handles = [plt.scatter([], [], s=500*size, color='gray', alpha=0.4) for size in sizes]\n",
    "    legend = plt.legend(size_handles, size_labels, scatterpoints=1, \n",
    "                        labelspacing=1.5, title=\"Unit Area\", loc='center left', \n",
    "                        bbox_to_anchor=(1.05, 0.5))\n",
    "    legend.get_frame().set_boxstyle(\"square\", pad=0.5)  # Increase padding to extend the frame\n",
    "    # legend.get_frame().set_edgecolor(None)  # Remove the border of the legend box\n",
    "    \n",
    "    # Add value annotations\n",
    "    # for i, (entropy, label) in enumerate(zip(entropy_values, facies_labels)):\n",
    "    #     plt.annotate(f\"{entropy:.3f}\",\n",
    "    #                 (i, entropy),\n",
    "    #                 xytext=(0, 5),\n",
    "    #                 textcoords='offset points',\n",
    "    #                 ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format='svg', bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def load_facies_map_from_shapefile(shapefile_path, raster_resolution, gdb_layer_name=None, unit_name=None, bbox=None):\n",
    "    \"\"\"\n",
    "    Convert a facies shapefile to a raster map\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shapefile_path : str\n",
    "        Path to the shapefile containing facies polygons\n",
    "    raster_resolution : float\n",
    "        Resolution of the output raster in map units\n",
    "    gdb_layer_name : str, optional\n",
    "        Name of the layer to read from a geodatabase\n",
    "    unit_name : str, optional\n",
    "        Name of the column containing facies information\n",
    "    bbox : tuple, optional\n",
    "        Bounding box for the raster (minx, miny, maxx, maxy)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    facies_map : numpy.ndarray\n",
    "        2D array where each unique integer represents a different facies\n",
    "    transform : affine.Affine\n",
    "        Affine transformation for the raster\n",
    "    facies_names : dict\n",
    "        Dictionary mapping facies IDs to their original names\n",
    "    \"\"\"\n",
    "    # Load the shapefile\n",
    "    if gdb_layer_name:\n",
    "        gdf = gpd.read_file(shapefile_path, layer=gdb_layer_name)\n",
    "    else:\n",
    "        gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    # Check if both \"crater rim\" and \"crater riim\" exist in the specified column\n",
    "    if \"Crater riim unit\" in gdf[unit_name].values and \"Crater rim unit\" in gdf[unit_name].values:\n",
    "        # Replace all occurrences of \"crater riim\" with \"crater rim\"\n",
    "        gdf[unit_name] = gdf[unit_name].replace(\"Crater riim unit\", \"Crater rim unit\")\n",
    "\n",
    "    # Add a 'facies_id' column to the GeoDataFrame\n",
    "    # Assigning unique IDs to each row\n",
    "    facies_names = {}\n",
    "    \n",
    "    if unit_name is not None:\n",
    "        # Create a mapping of category to ID\n",
    "        categories = gdf[unit_name].astype('category')\n",
    "        id_to_category = dict(enumerate(categories.cat.categories))\n",
    "        \n",
    "        # Assign category codes (starting from 1 to avoid 0)\n",
    "        gdf['facies_id'] = categories.cat.codes + 1\n",
    "        \n",
    "        # Create a mapping from facies_id to original name\n",
    "        for facies_id, original_name in id_to_category.items():\n",
    "            facies_names[facies_id + 1] = original_name\n",
    "    else:\n",
    "        # If no unit_name provided, use row numbers as IDs\n",
    "        gdf['facies_id'] = range(1, len(gdf) + 1)\n",
    "        \n",
    "        # Use row index as the name if no specific column is provided\n",
    "        for i in range(len(gdf)):\n",
    "            facies_names[i + 1] = f\"Facies {i + 1}\"\n",
    "    \n",
    "    # If no bbox is provided, use the bounds of the shapefile\n",
    "    if bbox is None:\n",
    "        bbox = gdf.total_bounds\n",
    "    \n",
    "    # Calculate raster dimensions\n",
    "    width = int((bbox[2] - bbox[0]) / raster_resolution)\n",
    "    height = int((bbox[3] - bbox[1]) / raster_resolution)\n",
    "    \n",
    "    # Create transform\n",
    "    transform = rasterio.transform.from_bounds(\n",
    "        bbox[0], bbox[1], bbox[2], bbox[3], width, height\n",
    "    )\n",
    "    \n",
    "    # Create a list of (geometry, value) pairs for rasterization\n",
    "    shapes = [(geom, value) for geom, value in zip(gdf.geometry, gdf['facies_id'])]\n",
    "    \n",
    "    # Rasterize the shapefile\n",
    "    facies_map = rasterize(\n",
    "        shapes,\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        all_touched=True,\n",
    "        fill=0,  # Background value\n",
    "        dtype=np.int32  # Use np.int32 to handle larger range of values\n",
    "    )\n",
    "    \n",
    "    return facies_map, transform, facies_names\n",
    "\n",
    "def main(base_image_path, facies_shape_path, output_prefix=None, raster_resolution=10.0, \n",
    "         kernel_radius=100.0, gdb_layer_name=None, unit_name=None, base_img_nan_value=None):\n",
    "    \"\"\"\n",
    "    Main workflow for facies complexity analysis using base image data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_image_path : str\n",
    "        Path to the base image (elevation or grayscale data)\n",
    "    facies_shape_path : str\n",
    "        Path to the shapefile containing facies polygons\n",
    "    output_prefix : str, optional\n",
    "        Prefix for output files\n",
    "    raster_resolution : float\n",
    "        Resolution of the raster in map units\n",
    "    kernel_radius : float\n",
    "        Radius of the kernel for calculating complexity in map units\n",
    "    gdb_layer_name : str, optional\n",
    "        Name of the layer to read from a geodatabase\n",
    "    unit_name : str, optional\n",
    "        Name of the column containing facies information\n",
    "    \"\"\"\n",
    "    # Load base image\n",
    "    print(\"Loading base image...\")\n",
    "    base_image, base_transform = load_base_image(base_image_path, img_nan_value = base_img_nan_value)\n",
    "    \n",
    "    # Load facies map\n",
    "    print(\"Loading facies map...\")\n",
    "    facies_map, facies_transform, facies_names = load_facies_map_from_shapefile(\n",
    "        facies_shape_path,\n",
    "        raster_resolution=raster_resolution,\n",
    "        gdb_layer_name=gdb_layer_name,\n",
    "        unit_name=unit_name,\n",
    "        bbox=(base_transform[2], base_transform[5] + base_image.shape[0] * base_transform[4],\n",
    "              base_transform[2] + base_image.shape[1] * base_transform[0], base_transform[5])\n",
    "    )\n",
    "    \n",
    "    # Print facies names\n",
    "    print(\"\\nFacies Names:\")\n",
    "    for facies_id, name in facies_names.items():\n",
    "        count = np.sum(facies_map == facies_id)\n",
    "        area = count * (raster_resolution ** 2)\n",
    "        print(f\"  Facies {facies_id}: {name} - {count} pixels ({area:.2f} square units)\")\n",
    "    \n",
    "    # Ensure base_image and facies_map have the same dimensions\n",
    "    if base_image.shape != facies_map.shape:\n",
    "        print(f\"Resizing base image from {base_image.shape} to match facies map {facies_map.shape}...\")\n",
    "        # Resample base_image to match facies_map dimensions\n",
    "        from rasterio.warp import reproject, Resampling\n",
    "        base_image_resampled = np.zeros(facies_map.shape, dtype=base_image.dtype)\n",
    "        reproject(\n",
    "            source=base_image,\n",
    "            destination=base_image_resampled,\n",
    "            src_transform=base_transform,\n",
    "            dst_transform=facies_transform,\n",
    "            src_crs=rasterio.CRS.from_epsg(9122),  \n",
    "            dst_crs=rasterio.CRS.from_epsg(9122),\n",
    "            resampling=Resampling.cubic\n",
    "        )\n",
    "        base_image = base_image_resampled\n",
    "    \n",
    "    # Calculate Shannon entropy for each facies\n",
    "    print(\"Calculating intra-facies entropy...\")\n",
    "    facies_entropy, entropy_map, facies_entropy_maps = calculate_facies_entropy(base_image, facies_map, norm=True)\n",
    "    \n",
    "    # Calculate complexity map that incorporates facies entropy\n",
    "    print(\"Calculating inter-facies complexity...\")\n",
    "    complexity_map = calculate_complexity_map(\n",
    "        facies_map, \n",
    "        facies_entropy, \n",
    "        pixel_size=raster_resolution, \n",
    "        kernel_radius=kernel_radius\n",
    "    )\n",
    "    \n",
    "    # Plot results with basemap overlay\n",
    "    plot_facies_and_complexity(facies_map, complexity_map, entropy_map, base_image, \n",
    "                              facies_transform, facies_names, title=\"Geo Unit Complexity Analysis\", save_path='jezero_ctx_complexity_fig.svg')\n",
    "    \n",
    "    # Plot facies entropy summary with original names\n",
    "    plot_facies_entropy_summary(facies_map, facies_entropy, facies_names, save_path='jezero_ctx_entropy_summary_fig.svg')\n",
    "    \n",
    "    # Save results if output_prefix is provided\n",
    "    if output_prefix:\n",
    "        # Save the entropy and complexity maps\n",
    "        output_meta = {\n",
    "            'driver': 'GTiff',\n",
    "            'height': facies_map.shape[0],\n",
    "            'width': facies_map.shape[1],\n",
    "            'count': 1,\n",
    "            'dtype': 'float32',\n",
    "            'crs': rasterio.CRS.from_epsg(4326),  # Assuming WGS84, adjust as needed\n",
    "            'transform': facies_transform\n",
    "        }\n",
    "        \n",
    "        with rasterio.open(f\"{output_prefix}_entropy.tif\", 'w', **output_meta) as dst:\n",
    "            dst.write(entropy_map.astype(np.float32), 1)\n",
    "        \n",
    "        with rasterio.open(f\"{output_prefix}_complexity.tif\", 'w', **output_meta) as dst:\n",
    "            dst.write(complexity_map.astype(np.float32), 1)\n",
    "        \n",
    "        print(f\"Results saved to {output_prefix}_entropy.tif and {output_prefix}_complexity.tif\")\n",
    "    \n",
    "    # Print summary statistics with original names\n",
    "    print(\"\\nFacies Entropy Summary:\")\n",
    "    for facies_id, entropy in facies_entropy.items():\n",
    "        if facies_id in facies_names:\n",
    "            name = facies_names[facies_id]\n",
    "            print(f\"Facies {facies_id} ({name}): Shannon entropy = {entropy:.4f}\")\n",
    "        else:\n",
    "            print(f\"Facies {facies_id}: Shannon entropy = {entropy:.4f}\")\n",
    "    \n",
    "    print(\"\\nComplexity Map Summary:\")\n",
    "    print(f\"Mean complexity: {np.mean(complexity_map):.4f}\")\n",
    "    print(f\"Max complexity: {np.max(complexity_map):.4f}\")\n",
    "    print(f\"Min complexity: {np.min(complexity_map):.4f}\")\n",
    "    \n",
    "    return facies_map, entropy_map, complexity_map, facies_entropy, facies_names \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # Replace these paths with your actual data paths\n",
    "    gdb_path = '/mnt/nili_e/Software/fractal_complexity/data/sim3464_JezeroNili_FINAL/Geodatabase/Nili.gdb'\n",
    "    layer_name = 'GeoUnits_v1'\n",
    "    base_img_path = '/mnt/nili_e/Software/fractal_complexity/data/Jezero_HiRISE_clipped_tiles/Jezero_HiRISE_clipped_tile_30500_54500.tif'\n",
    "    # base_img_path = '/Users/phillipsm/Documents/Research/Proposals/2024/SSW/COMPLEX/sim3464_JezeroNili_FINAL/Rasters/HRSC/nili_h0988_0000_da4_clip.tif'\n",
    "\n",
    "    facies_map, entropy_map, complexity_map, facies_entropy, facies_names = main(\n",
    "        base_image_path=base_img_path,\n",
    "        facies_shape_path=gdb_path,\n",
    "        raster_resolution=0.25,\n",
    "        kernel_radius=10.0,\n",
    "        gdb_layer_name=layer_name,\n",
    "        unit_name=\"name\",  # Column name containing facies information in the shapefile\n",
    "        # base_img_nan_value = -32768\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
